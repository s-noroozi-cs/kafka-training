docker run -d --rm -p 9092:9092 apache/kafka:3.7.0

./kafka-topics.sh --bootstrap-server localhost:9092 --topic test --partitions 3 --create

./kafka-topics.sh --bootstrap-server localhost:9092 --describe

./kafka-producer-perf-test.sh --topic test \
                              --throughput 1000 --num-records 3000000 \
                              --record-size 1024 \
                              --producer-props bootstrap.servers=localhost:9092

./kafka-console-consumer.sh --consumer-property group.id=test-consumer-group \
	  		--consumer-property client.id=test-client-id-1 \
	  		--consumer-property enable.auto.commit=true \
	  		--topic test --bootstrap-server localhost:9092

./kafka-console-consumer.sh --consumer-property group.id=test-consumer-group \
	  		--consumer-property client.id=test-client-id-2 \
	  		--consumer-property enable.auto.commit=true \
	  		--topic test --bootstrap-server localhost:9092

./kafka-console-consumer.sh --consumer-property group.id=test-consumer-group \
	  		--consumer-property client.id=test-client-id-3 \
	  		--consumer-property enable.auto.commit=true \
	  		--topic test --bootstrap-server localhost:9092

./kafka-console-consumer.sh --consumer-property group.id=test-consumer-group \
	  		--consumer-property client.id=test-client-id-4 \
	  		--consumer-property enable.auto.commit=true \
	  		--topic test --bootstrap-server localhost:9092



parameters to tuning 

  optimizing consumer throughput and latency
      # ...
      fetch.max.wait.ms=500
      fetch.min.bytes=16384
      # ...

  avoiding data loss or duplication
      # ...
      enable.auto.commit=false
      # ...

  controlling transactional messages

    on the producer side to guarantee exactly-once delivery

      Consider using transactional ids and enabling idempotence 
          (enable.idempotence=true) 
          
    On the consumer side, 
      
        you can then use the isolation.level property 
        
          to control how transactional messages are read by the consumer.

          The isolation.level property has two valid values:

            read_committed
            read_uncommitted (default)

        Set to read_committed so that only committed messages are read by the consumer.

  Managing offset policy

    # ...
    heartbeat.interval.ms=3000
    # Adjust the heartbeat interval lower according to anticipated rebalances.

    session.timeout.ms=45000
    # If no heartbeats are received by the Kafka broker before the timeout duration expires, 
    # the consumer is removed from the consumer group and a rebalance is initiated. 
    # If the broker configuration has a group.min.session.timeout.ms and group.max.session.timeout.ms, 
    # the session timeout value must be within that range.


    auto.offset.reset=earliest
    # Set to earliest to return to the start of a partition and avoid data loss if offsets were not committed.
    # ...

    Suppose you deploy a consumer application for the first time, and it reads messages from an existing topic. 
    Because this is the first time the group.id is used, the __consumer_offsets topic does not contain any offset 
    information for this application. The new application can start processing all existing messages from the start 
    of the log or only new messages. The default reset value is latest, which starts at the end of the partition, 
    and consequently means some messages are missed. To avoid data loss, but increase the amount of processing, 
    set auto.offset.reset to earliest to start at the beginning of the partition.


  Minimizing the impact of rebalances

  The rebalancing of a partition between active consumers in a group is the time it takes for:

    Consumers to commit their offsets
    The new consumer group to be formed
    The group leader to assign partitions to group members
    The consumers in the group to receive their assignments and start fetching


    # ...
      group.instance.id=UNIQUE-ID
      # The unique instance id ensures that a new consumer instance receives the same assignment of topic partitions.

      max.poll.interval.ms=300000
      # Set the interval to check the consumer is continuing to process messages.

      max.poll.records=500
      # Sets the number of processed records returned from the consumer.

    # ...


    decreasing replica size

    1. start kafka cluster with 3 node
      
      docker-compose -f docker-compose-kafka-cluster.yml up

    2. create topic with replication factor 3

        open your browser and type localhost:8080
        
        choose topics section form left menu and create test topic with 3 partitions and 3 replicas

    3. using kafka-partiotion-reassignment tools to create current state of topic

        got terminal from one of the kafka container - for example kafka-1

        change current directory to /opt/kafka/bin

        create file with name topics.json with following content

           {"version": 1,"topics": [{ "topic": "test"}]}

        ./kafka-reassign-partitions.sh \
                --bootstrap-server kafka-1:9092 \
                --topics-to-move-json-file topics.json \
                --broker-list 1,2,3 \
                --generate

        copy Proposed partition reassignment configuration section to file with name test-new-assignment.json

          {"version":1,"partitions":[{"topic":"test","partition":0,"replicas":[3,2,1],"log_dirs":["any","any","any"]},{"topic":"test","partition":1,"replicas":[1,3,2],"log_dirs":["any","any","any"]},{"topic":"test","partition":2,"replicas":[2,1,3],"log_dirs":["any","any","any"]}]}
          
        also for backup flow copy Current partition replica assignment to file with name test-origin-state.json
          
          {"version":1,"partitions":[{"topic":"test","partition":0,"replicas":[3,1,2],"log_dirs":["any","any","any"]},{"topic":"test","partition":1,"replicas":[1,2,3],"log_dirs":["any","any","any"]},{"topic":"test","partition":2,"replicas":[2,3,1],"log_dirs":["any","any","any"]}]}


    4. changing replica section based on your requierment 

        change assignment json file , based on your requirment - change replication section - 

            in our cases, we change replication factor from 3 to 2

        {"version":1,"partitions":[{"topic":"test","partition":0,"replicas":[3,2],"log_dirs":["any","any"]},{"topic":"test","partition":1,"replicas":[1,3],"log_dirs":["any","any"]},{"topic":"test","partition":2,"replicas":[2,1],"log_dirs":["any","any"]}]}

    5. using kafka-partiotn-reassignment tools to execute your target changes

                    ./kafka-reassign-partitions.sh \
                        --bootstrap-server kafka-1:9092 \
                        --reassignment-json-file test-new-assignment.json \
                        --execute

                please verify your changes

                    ./kafka-reassign-partitions.sh \
                        --bootstrap-server kafka-1:9092 \
                        --reassignment-json-file test-new-assignment.json \
                        --verify


    6. we do it :-)



    log retention and clean up policies

        support two type of retention polocies
          
          time based

            log.retention.ms
            log.retention.minutes
            log.retention.hours

          size based

            log.retention.bytes

    clean up policy
      
      log.cleanup.policy
      # default value is delete
      # valid values are delete and compact

    

    cat /opt/kafka/config/server.properties  | grep log

      please check segment size



scenario compact

  KAFKA_LOG_CLEANUP_POLICY=compact

    1-solution A
    KAFKA_LOG_SEGMENT_BYTES=70
    KAFKA_LOG_CLEANER_MIN_CLEANABLE_RATIO=0.01

    2-solution B
    KAFKA_LOG_CLEANER_MAX_COMPACTION_LAG_MS=100

scenario retention

    KAFKA_LOG_RETENTION_MS=3000

    KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS=10000

docker run --rm -it -p 9092:9092  \
  --name broker \
  -e KAFKA_NODE_ID=1 \
  -e KAFKA_PROCESS_ROLES=broker,controller \
  -e KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \
  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
  -e KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER \
  -e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \
  -e KAFKA_CONTROLLER_QUORUM_VOTERS=1@localhost:9093 \
  -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
  -e KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \
  -e KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1 \
  -e KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0 \
  
  apache/kafka:3.7.0

