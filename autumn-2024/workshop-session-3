https://developers.redhat.com/articles/2024/03/13/kafka-tiered-storage-deep-dive

https://developers.redhat.com/articles/2023/11/22/getting-started-tiered-storage-apache-kafka

https://www.kai-waehner.de/blog/2023/12/05/why-tiered-storage-for-apache-kafka-is-a-big-thing/

https://docs.confluent.io/platform/current/clusters/tiered-storage.html

https://aiven.io/developer/kafka-tiered-storage-terraform



Traditional Kafka deployments often rely on 
    direct-attached storage (DAS) or 
    network-attached storage (NAS) 
for storing the data.

The idea is to use different storage layers based on the characteristics of the data

In a Kafka cluster, not all data is created equal. Some data, 
    referred to as 
        
        “hot” data, is frequently accessed and needs to be readily available for quick retrieval. 
        Hot Tier (SSD/NVMe)
        high-performance and low-latency access
        data most critical for real-time processing is readily available
        Specify Hot Tier Directory
            # Server Properties for Hot Tier
            log.dirs=/path/to/hot/tier

    On the other hand, 
        
        “cold” data, which is less frequently accessed, can be moved to a more cost-effective and scalable storage solution
        Cold Tier (HDD/S3)
        designed for cost efficiency and scalability
        historical and less frequently accessed data
        organizations can reduce storage costs
        maintaining the ability to access historical information

Apache Kafka’s Tiered Storage is a data management strategy 
    that categorizes storage into two tiers i.e. local and remote

Without support of tiered storage,  a Kafka cluster’s storage can be scaled by 
    adding more broker nodes or 
    replacing current nodes with higher capacity nodes. 
It is not a cost-effective way of increasing storage. 
    Adding new nodes would also require copying a lot of data 
        that makes operations difficult and time consuming.

Enablement of Tiered Storage on Apache Kafka cluster 
    does not change the way producers and consumers interact with each other, 
    it only impacts the data retention and data retrieval process.

producer perspective 
    In situations such as high data ingestion or temporary errors like network connectivity issues, 
    the local storage may temporarily exceed the specified local retention threshold, 
    resulting in an accumulation of additional data in local tier. 
    The log cleaner will not remove this data until it has been successfully uploaded to remote storage.

consumer perspective
    For data retrieval consumer requests on Tiered Storage enabled topics, 
    if the data is available in local storage, it is served from the local disk. 
    If the requested data sits in the remote storage, the broker streams the data from 
    it into its in-memory buffer (and on-disk cache), and then sends it back to the client.

 

    





